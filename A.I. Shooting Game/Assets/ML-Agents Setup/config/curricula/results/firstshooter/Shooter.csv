Steps,Policy/Entropy,Policy/Extrinsic Value Estimate,Environment/Episode Length,Environment/Cumulative Reward,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate,Policy/Epsilon,Policy/Beta,Is Training
50000,1.4173577,-0.25396344,71.75655976676384,-1.0602343242700991,-1.0602343242700991,0.06044497,0.023336053,0.00028461247,0.1948708,0.0047440524,1.0
100000,1.4108951,-0.34551337,614.85,-1.6104260003528907,-1.6104260003528907,0.007889813,0.02329975,0.00025685687,0.18561895,0.0042823856,1.0
150000,1.402933,-0.24616086,1036.8297872340424,-1.522891065259365,-1.522891065259365,0.0033799042,0.021821842,0.00022604523,0.17534843,0.0037698853,1.0
200000,1.3967142,-0.17288145,1479.5142857142857,-1.2599600067561758,-1.2599600067561758,0.0016538047,0.023630584,0.00019524172,0.16508056,0.0032575198,1.0
250000,1.3933803,-0.12123135,1575.225806451613,-0.8266451797943802,-0.8266451797943802,0.0011211889,0.023788016,0.00016441866,0.15480618,0.0027448293,1.0
300000,1.3906753,-0.07097312,973.3269230769231,0.024921471763473865,0.024921471763473865,0.0017198271,0.023460263,0.00013357495,0.14452496,0.0022317958,1.0
350000,1.3876697,0.058694057,194.0653846153846,0.6121350304559808,0.6121350304559808,0.012714855,0.02399512,0.00010272212,0.13424069,0.0017186098,1.0
